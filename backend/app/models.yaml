meta_model:
  # The "Chief Editor" - A stronger model to aggregate results
  # If you have 24GB+ VRAM, use "Qwen/Qwen2.5-14B-Instruct" for even better results.
  id: "meta-llama/Meta-Llama-3.1-8B-Instruct" 
  max_tokens: 1024
  temperature: 0.2

tasks:
  math:
    # 1.5B-3B Math Specialists
    default_max_tokens: 512
    default_temperature: 0.6
    models:
      - id: "Qwen/Qwen2.5-Math-1.5B-Instruct" # SOTA for its size
      - id: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" # Excellent reasoning
      - id: "microsoft/Phi-3-mini-4k-instruct" # Strong math/logic capabilities
      - id: "Qwen/Qwen2.5-1.5B-Instruct"
      - id: "meta-llama/Llama-3.2-3B-Instruct"

  code:
    # 1.5B-3B Coding Specialists
    default_max_tokens: 1024
    default_temperature: 0.2
    models:
      - id: "Qwen/Qwen2.5-Coder-1.5B-Instruct" # Specialized coding model
      - id: "Qwen/Qwen2.5-Coder-3B-Instruct"
      - id: "deepseek-ai/deepseek-coder-1.3b-instruct"
      - id: "ibm-granite/granite-3b-code-instruct"
      - id: "meta-llama/Llama-3.2-3B-Instruct"

  research:
    # Generalist 2B-3B models for synthesis
    default_max_tokens: 512
    default_temperature: 0.3
    models:
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "Qwen/Qwen2.5-3B-Instruct"
      - id: "google/gemma-2-2b-it"
      - id: "microsoft/Phi-3-mini-4k-instruct"
      - id: "HuggingFaceTB/SmolLM2-1.7B-Instruct"

  qa:
    default_max_tokens: 256
    default_temperature: 0.5
    models:
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "Qwen/Qwen2.5-3B-Instruct"
      - id: "google/gemma-2-2b-it"
      - id: "microsoft/Phi-3-mini-4k-instruct"
      - id: "Qwen/Qwen2.5-1.5B-Instruct"

  creative:
    default_max_tokens: 768
    default_temperature: 0.8
    models:
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "Qwen/Qwen2.5-3B-Instruct"
      - id: "google/gemma-2-2b-it"
      - id: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
      - id: "microsoft/Phi-3-mini-4k-instruct"

  summary:
    default_max_tokens: 512
    default_temperature: 0.3
    models:
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "Qwen/Qwen2.5-3B-Instruct"
      - id: "google/gemma-2-2b-it"
      - id: "microsoft/Phi-3-mini-4k-instruct"
      - id: "Qwen/Qwen2.5-1.5B-Instruct"

  business:
    default_max_tokens: 512
    default_temperature: 0.5
    models:
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "Qwen/Qwen2.5-3B-Instruct"
      - id: "google/gemma-2-2b-it"
      - id: "microsoft/Phi-3-mini-4k-instruct"
      - id: "meta-llama/Llama-3.2-1B-Instruct"

  data:
    default_max_tokens: 512
    default_temperature: 0.1
    models:
      - id: "Qwen/Qwen2.5-Math-1.5B-Instruct"
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "microsoft/Phi-3-mini-4k-instruct"
      - id: "Qwen/Qwen2.5-Coder-1.5B-Instruct"
      - id: "google/gemma-2-2b-it"

  translation:
    default_max_tokens: 512
    default_temperature: 0.3
    models:
      - id: "Qwen/Qwen2.5-3B-Instruct" # Qwen is excellent at multilingual tasks
      - id: "google/gemma-2-2b-it"
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "microsoft/Phi-3-mini-4k-instruct"
      - id: "HuggingFaceTB/SmolLM2-1.7B-Instruct"

  chat:
    default_max_tokens: 512
    default_temperature: 0.7
    models:
      - id: "meta-llama/Llama-3.2-3B-Instruct"
      - id: "Qwen/Qwen2.5-3B-Instruct"
      - id: "google/gemma-2-2b-it"
      - id: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
      - id: "microsoft/Phi-3-mini-4k-instruct"